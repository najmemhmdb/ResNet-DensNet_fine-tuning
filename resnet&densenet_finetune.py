# -*- coding: utf-8 -*-
"""hw4_99131009.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYZhs9hLfRKrxFtWgTL5fWuslnPrMmKH
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import math
from PIL import Image
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.layers import Input, Dense, Flatten, Dropout, LeakyReLU, Activation
from keras.layers.normalization import BatchNormalization
from keras.applications.densenet import DenseNet201
from tensorflow import keras
from sklearn.metrics import confusion_matrix

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

# load indoor data
os.chdir('/content/drive/My Drive/ANN/hw4/indoor')
listOfFiles_indoor = os.listdir('.')
X_in = []
for indoor in listOfFiles_indoor:
    X_in.append(np.asarray(Image.open(indoor).resize((128, 128))))
    
X_in = np.array(X_in) / 255

# load outdoor data
os.chdir('/content/drive/My Drive/ANN/hw4/outdoor')
listOfFiles_outdoor = os.listdir('.')
X_out = []
for outdoor in listOfFiles_outdoor:
    X_out.append(np.asarray(Image.open(outdoor).resize((128, 128))))
X_out = np.array(X_out) / 255

# shuffle data
X = []
y = []
for i in range(400):
    X.append(X_in[i])
    y.append(0)
    X.append(X_out[i])
    y.append(1)
X = np.array(X)
y = np.array(y)
os.chdir('/content/drive/My Drive/ANN/hw4')

# split data 
X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.3, random_state=42)
X_valid,X_test,y_valid,y_test = train_test_split(X_rest,y_rest,test_size= 0.33,random_state=42)

"""**DenseNET**"""

# Inputs
inputs = Input(shape=(128,128,3))

# DenseNet
densenet201 = DenseNet201(weights='imagenet', include_top=False)(inputs)

# densenet201 = DenseNet201()
# densenet201.summary()

# Our FC layer
flat1 = Flatten()(densenet201)
dense1 = Dense(units=256, use_bias=True)(flat1)
batchnorm1 = BatchNormalization()(dense1)

# Output
out = Dense(units=1, activation='sigmoid')(batchnorm1)

# Create Model
model = Model(inputs=inputs, outputs=out)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001), loss='binary_crossentropy',metrics=['accuracy'])

keras.utils.plot_model(model,show_shapes= True, expand_nested=True)

"""**resNET**"""

# ResNet

 resnet152 = keras.applications.ResNet152(include_top=False, weights='imagenet',input_tensor=None,input_shape=(128,128,3),pooling=None)
resnet152.trainable = False
model=keras.models.Sequential()
model.add(resnet152)
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(1024,activation='relu'))
model.add(keras.layers.Dense(64,activation='relu'))
model.add(keras.layers.Dense(2))
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])

for layer in model.layers[0:-50]:
   layer.trainable = False

for layer in model.layers[-50:-1]:
    layer.trainable = True

class EarlyStoppingCallback(keras.callbacks.Callback):
  def __init__(self,patience=0):
    super(EarlyStoppingCallback,self).__init__()
    self.patience = patience
    self.val_acc = []
    self.trn_acc = []
    self.cur_epoch = 0

  def on_train_begin(self,logs=None):
    self.best = -1 * np.Inf
    self.wait  = 0
    self.stopped_epoch = 0
  def on_epoch_end(self,epoch,logs =None):
    current_acc = logs.get("val_accuracy")
    self.val_acc.append(current_acc)
    self.trn_acc.append(logs.get("accuracy"))
    self.cur_epoch = epoch
    if np.greater(current_acc,self.best):
      self.best = current_acc
      self.wait = 0
      print("******************")
      print(current_acc)
      self.best_weights = self.model.get_weights()
    else: 
      self.wait +=1 
      if self.wait > self.patience :
        print("in stop scope")
        self.stopped_epoch = epoch - self.patience
        self.model.stop_training = True
        self.model.set_weights(self.best_weights)
  def on_train_end(self,logs = None):
    if self.stopped_epoch > 0 :
      print("epoch: %d: early stopping" , self.cur_epoch)
      print(self.best)
    plt.plot(range(self.cur_epoch+1),self.val_acc,label="val_acc",color = 'orange')
    plt.plot(range(self.cur_epoch+1),self.trn_acc,label="train_acc", color='blue')
    plt.legend()
    plt.show()

er_cal = EarlyStoppingCallback(patience=5)
model.fit(X_train, y_train,epochs= 100, validation_data=(np.array(X_valid),np.array(y_valid)),callbacks=[er_cal])

test_loss,test_acc = model.evaluate(X_test,y_test,verbose=2)
predicted = model.predict(X_test)

y_pred = [np.argmax(predicted[i]) for i in range(len(predicted))]
print(y_pred)
y_pred = []
for i in range(len(predicted)):
    if predicted[i] > 0.5:
       y_pred.append(1)
    else:
       y_pred.append(0)
confusion_matrix(y_test, y_pred)